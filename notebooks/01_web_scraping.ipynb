{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941b704a",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Este script demuestra cómo hacer web scraping usando Playwright para extraer información de productos de una tienda online de ejemplo.\n",
    "\n",
    "### Requisitos:\n",
    "- Instalar las dependencias necesarias:\n",
    "  > pip install pandas requests beautifulsoup4\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ced013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09efb48",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ae3a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.gsmarena.com/makers.php3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Uso\u001b[39;00m\n\u001b[32m     61\u001b[39m scraper = GSMArenaScraper()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m brands = \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m brands:\n\u001b[32m     64\u001b[39m     devices = scraper.scrape_brand_devices(b[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mGSMArenaScraper.scrape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     33\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Método principal para ejecutar el scraping\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m.parse_brands(html)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.brands\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mGSMArenaScraper.fetch_page\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Realiza la solicitud HTTP y devuelve el contenido HTML\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m response = requests.get(\u001b[38;5;28mself\u001b[39m.url)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Proyectos\\Proyectos GitHub\\gsmarena\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 429 Client Error: Too Many Requests for url: https://www.gsmarena.com/makers.php3"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "class GSMArenaScraper:\n",
    "    \"\"\"\n",
    "    Clase para realizar web scraping de la página de fabricantes de GSMArena.\n",
    "\n",
    "    Atributos:\n",
    "        url (str): URL de la página de fabricantes.\n",
    "        brands (list): Lista para almacenar la información de las marcas.\n",
    "        session (requests.Session): Sesión HTTP para reutilizar conexiones.\n",
    "        request_count (int): Contador de peticiones realizadas.\n",
    "        pause_every (int): Número de peticiones después de las cuales se realiza una pausa.\n",
    "        pause_time_range (tuple): Rango de segundos para pausar (min, max).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url='https://www.gsmarena.com/makers.php3', pause_every=10, pause_time_range=(5, 10)):\n",
    "        \"\"\"\n",
    "        Inicializa el scraper.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL inicial de fabricantes de GSMArena.\n",
    "            pause_every (int): Número de peticiones tras las cuales se pausa.\n",
    "            pause_time_range (tuple): Rango de tiempo aleatorio para pausar.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.brands = []\n",
    "        self.session = requests.Session()\n",
    "        self.request_count = 0\n",
    "        self.pause_every = pause_every\n",
    "        self.pause_time_range = pause_time_range\n",
    "\n",
    "    def _check_pause(self):\n",
    "        \"\"\"\n",
    "        Verifica si se debe pausar el scraping según el número de peticiones realizadas.\n",
    "        Introduce un retraso aleatorio entre peticiones para evitar bloqueos.\n",
    "        \"\"\"\n",
    "        if self.request_count > 0 and self.request_count % self.pause_every == 0:\n",
    "            wait_time = random.randint(*self.pause_time_range)\n",
    "            print(f\"Se alcanzó {self.request_count} peticiones. Pausando {wait_time} segundos...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    def fetch_page(self, url=None):\n",
    "        \"\"\"\n",
    "        Realiza la solicitud HTTP y devuelve el contenido HTML de una página.\n",
    "\n",
    "        Args:\n",
    "            url (str, opcional): URL a solicitar. Si no se proporciona, se usa la URL principal.\n",
    "\n",
    "        Returns:\n",
    "            str: Contenido HTML de la página.\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            url = self.url\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        self.request_count += 1\n",
    "        self._check_pause()\n",
    "        return response.text\n",
    "\n",
    "    def parse_brands(self, html):\n",
    "        \"\"\"\n",
    "        Analiza el HTML y extrae información de marcas.\n",
    "\n",
    "        Args:\n",
    "            html (str): Contenido HTML de la página de fabricantes.\n",
    "\n",
    "        Actualiza:\n",
    "            self.brands: lista de diccionarios con 'name', 'url' y 'devices'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            makers_container = soup.find('div', class_='st-text')\n",
    "            brand_links = makers_container.find_all('a')\n",
    "\n",
    "            for link in brand_links:\n",
    "                brand_name = link.contents[0].strip()\n",
    "                brand_url = 'https://www.gsmarena.com/' + link.get('href')\n",
    "                devices_text = link.find('span').get_text(strip=True)\n",
    "                devices_number = int(re.search(r'\\d+', devices_text).group())\n",
    "                self.brands.append({\n",
    "                    'name': brand_name,\n",
    "                    'url': brand_url,\n",
    "                    'devices': devices_number\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f'Error al analizar el HTML: {e}')\n",
    "\n",
    "    def scrape(self):\n",
    "        \"\"\"\n",
    "        Método principal para ejecutar el scraping de marcas.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de diccionarios con información de las marcas.\n",
    "        \"\"\"\n",
    "        html = self.fetch_page()\n",
    "        self.parse_brands(html)\n",
    "        return self.brands\n",
    "\n",
    "    def scrape_brand_devices(self, brand_url):\n",
    "        \"\"\"\n",
    "        Extrae la lista de dispositivos de una marca específica.\n",
    "\n",
    "        Args:\n",
    "            brand_url (str): URL de la página de la marca.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de diccionarios con 'name' y 'url' de cada dispositivo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            html = self.fetch_page(brand_url)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            devices = []\n",
    "            device_links = soup.find_all('div', class_='makers')[0].find_all('a')\n",
    "\n",
    "            for link in device_links:\n",
    "                device_name = link.find('strong').get_text(strip=True)\n",
    "                device_url = 'https://www.gsmarena.com/' + link.get('href')\n",
    "                devices.append({'name': device_name, 'url': device_url})\n",
    "            return devices\n",
    "        except Exception as e:\n",
    "            print(f'Error al raspar dispositivos: {e}')\n",
    "            return []\n",
    "\n",
    "# Uso del scraper\n",
    "scraper = GSMArenaScraper(pause_every=5, pause_time_range=(5, 10))\n",
    "brands = scraper.scrape()\n",
    "for b in brands:\n",
    "    devices = scraper.scrape_brand_devices(b['url'])\n",
    "    print(f\"Dispositivos de {b['name']}:\")\n",
    "    for d in devices:\n",
    "        print(f\" - {d['name']}: {d['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "922c473a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GSMArenaScraper at 0x1f7d1a98f50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efc144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
